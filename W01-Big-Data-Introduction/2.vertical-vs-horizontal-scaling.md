# Vertical vs Horizontal Scaling

Big Data is partially defined by the issue that large datasets pose on more traditional database systems, such as relational databases. In this introduction we will explore these two important concepts in relation to traditional RDBM's and Big Data systems along the dimensions of vertical and horizontal scaling.

- [Vertical vs Horizontal Scaling](#vertical-vs-horizontal-scaling)
  - [Vertical Scaling](#vertical-scaling)
    - [Technical Insights](#technical-insights)
    - [Advantages](#advantages)
    - [Limitations](#limitations)
  - [Horizontal Scaling](#horizontal-scaling)
    - [Technical Insights](#technical-insights-1)
      - [Infrastructure Components](#infrastructure-components)
      - [Data Handling and Processing](#data-handling-and-processing)
      - [Fault Tolerance and Reliability](#fault-tolerance-and-reliability)
      - [Scalability and Elasticity](#scalability-and-elasticity)
      - [Software Considerations](#software-considerations)
      - [Security and Compliance](#security-and-compliance)
      - [Performance Optimization](#performance-optimization)
      - [Monitoring and Maintenance](#monitoring-and-maintenance)
    - [Advantages](#advantages-1)
    - [Limitations](#limitations-1)
  - [Big Data Technologies and Horizontal Scalability](#big-data-technologies-and-horizontal-scalability)
  - [Conclusions](#conclusions)

## Vertical Scaling

Vertical scaling, often referred to as "scaling up," involves enhancing the capacity of a single server or node by adding more resources. This could mean increasing CPU power, adding more RAM, or expanding storage capacity. In the context of databases, vertical scaling typically involves upgrading the existing hardware of the server where the database resides.

### Technical Insights

- **Hardware Upgrades:** This includes enhancing CPUs (faster or more cores), increasing RAM for better data handling, and expanding disk storage for more data capacity.
- **Software Optimization:** Sometimes, vertical scaling also involves optimizing database software for better performance on powerful hardware.
Extended Advantages
- **Transactional Integrity:** Maintains the ACID properties (Atomicity, Consistency, Isolation, Durability) inherently due to a single-node architecture.
Reduced Complexity: Fewer nodes mean simpler management and maintenance of the database system.

### Advantages

- **Simplicity:** It's often simpler to scale a system vertically as it doesn't require significant changes to the software or architecture.
- **Consistency:** In RDBMS, vertical scaling maintains the consistency of data, as all transactions are managed by a single database instance.
- **Transactional Integrity:** Maintains the ACID properties (Atomicity, Consistency, Isolation, Durability) inherently due to a single-node architecture.
- **Reduced Complexity:** Fewer nodes mean simpler management and maintenance of the database system.

### Limitations

- **Physical Limits:** There's a ceiling to how much you can upgrade a single server. Beyond certain limits, you can't add more resources.
- **Cost and Downtime:** High-end hardware can be expensive. Additionally, scaling up often requires downtime while the server is being upgraded.
- **Single Point of Failure:** If the server fails, the entire system goes down.
- **Scalability Ceiling:** Hardware upgrades have a physical and economic limit.
Maintenance and Upgrade Challenges: Upgrading hardware often requires system downtime, impacting availability.

## Horizontal Scaling

Horizontal scaling, or "scaling out," involves adding more nodes to a system, such as additional servers in a database cluster. Instead of upgrading the capabilities of a single server, you distribute the load across multiple servers or nodes. This is arguably a much more complex architecture to manage - and therefore, the technological insights required to understand the issues surrounding horizontal scaling a much more involved that in vertical scaling solutions.

### Technical Insights

#### Infrastructure Components

1. **Cluster Management**: In horizontal scaling, managing a cluster of nodes is crucial. This involves orchestration tools that automate the deployment, scaling, and operation of nodes across the network.

2. **Load Balancing**: This is essential for distributing workloads evenly across all nodes. Effective load balancing ensures that no single node is overwhelmed, optimizing resource use and response times.

3. **Network Optimization**: High-speed and reliable network connectivity is vital to facilitate swift communication and data transfer between nodes, reducing latency and avoiding bottlenecks.

#### Data Handling and Processing

1. **Data Partitioning (Sharding)**: This involves dividing a larger database into smaller, more manageable pieces, or shards, each hosted on separate nodes. Sharding is key to enhancing performance and scalability in a horizontally scaled environment.

2. **Replication**: Ensures data availability and fault tolerance. In case of a node failure, replicated data on other nodes can be used to maintain continuous operation.

3. **Consistent Hashing**: Used in distributed caching systems to efficiently distribute and locate data across multiple nodes with minimal reorganization when nodes are added or removed.

#### Fault Tolerance and Reliability

1. **Redundancy**: Implementing redundancy in a horizontal scaling architecture is vital to ensure that the failure of a single node does not affect the overall system's availability.

2. **Automatic Failover**: Systems should be designed to automatically switch to a redundant or standby node in the event of a failure, minimizing downtime.

#### Scalability and Elasticity

1. **Dynamic Scaling**: The ability to add or remove nodes from the system dynamically, often in response to application load, without disrupting ongoing processes.

2. **Elasticity**: The system’s ability to scale resources up or down as needed, which is particularly important in cloud computing environments.

#### Software Considerations

1. **Distributed Computing Frameworks**: Frameworks like Apache Spark and Hadoop are designed to distribute and process data across multiple nodes effectively.

2. **Database Management**: Tools like Cassandra, MongoDB, Hive, and Impala provide mechanisms for managing data across distributed systems, ensuring efficient querying and data integrity.

#### Security and Compliance

1. **Data Encryption**: In a distributed environment, encrypting data in transit and at rest becomes crucial to protect sensitive information.

2. **Access Control**: Robust access control mechanisms ensure that only authorized users can access or modify data, critical in a distributed and potentially more exposed environment.

#### Performance Optimization

1. **Caching**: Implementing caching strategies to reduce database load and improve response times. Distributed caches store frequently accessed data in memory across multiple nodes.

2. **Query Optimization**: Optimizing queries to minimize cross-node data transfer, which can be a significant performance bottleneck in distributed systems.

#### Monitoring and Maintenance

1. **Real-Time Monitoring**: Continuously monitoring the health and performance of each node to preemptively identify and resolve issues.

2. **Automated Maintenance**: Implementing automated tools for routine maintenance tasks such as backups, updates, and health checks.

### Advantages

- **Flexibility:** It's easier to scale out incrementally by adding more servers as needed.
- **Resilience:** With multiple nodes, the failure of one doesn’t cripple the entire system.
- **Cost-Effectiveness:** Often, it's more cost-effective to add standard, off-the-shelf servers than to invest in high-end, specialized hardware.

### Limitations

- **Complexity in Data Distribution:** RDBMSs are traditionally designed for single-node setups. Distributing data across multiple nodes while maintaining ACID (Atomicity, Consistency, Isolation, Durability) properties is complex.
- **Join Operations:** Performing join operations across distributed databases can be highly inefficient and slow.
- **Data Integrity:** Ensuring data integrity and consistency across multiple nodes is challenging, especially under high transaction volumes.

## Big Data Technologies and Horizontal Scalability

**Hadoop, Spark, and NoSQL databases** have been designed with horizontal scalability in mind, making them more adept at handling large-scale data processing and storage compared to traditional RDBMS.

1. **Hadoop:**
   - **Distributed File System (HDFS):** Hadoop uses HDFS, which allows it to store massive amounts of data across many machines.
   - **MapReduce:** This programming model efficiently processes large data sets across a distributed cluster.
   - **Scalability:** Hadoop clusters can be scaled out by simply adding more nodes. It's designed to handle petabytes of data.

2. **Apache Spark:**
   - **In-Memory Processing:** Spark processes data in memory, which is faster than the disk-based processing used by Hadoop's MapReduce.
   - **Flexibility:** It can run on Hadoop, standalone, or in the cloud, and can access diverse data sources.
   - **Resilience:** Spark's Resilient Distributed Datasets (RDDs) provide fault tolerance through lineage information.

3. **NoSQL Databases:**
   - **Schema-less:** They don't require a fixed schema, allowing for more flexibility in storing different types of data.
   - **Built for Distribution:** Many NoSQL databases (like Cassandra, MongoDB) are designed from the ground up to be distributed.
   - **Different Data Models:** They support various data models (key-value, document, column-family, graph), making them suitable for a wide range of applications.

## Conclusions

In summary, while vertical scaling is often simpler and maintains the integrity and consistency of RDBMS, it has its limitations in terms of physical capacity, cost, and risk of a single point of failure. Horizontal scaling offers greater flexibility and resilience but introduces complexity in maintaining data integrity and performance in RDBMS. Big data technologies like Hadoop, Spark, and NoSQL databases, on the other hand, are inherently designed for horizontal scaling, allowing them to handle larger volumes of data more efficiently and reliably. This makes them particularly suitable for applications and workloads that require processing and storing large amounts of data. In this course we will introduce students to these aforementioned systems including Hadoop (HDFS and MapReduce), NoSQL (HBase, Cassandra, MongoDB), data warehousing technolgies such as Hive, and data processing a computational distribution using Spark.
